[Приклад роботи з Lakehouse архітектурою на основі AWS стеку](https://aws.amazon.com/blogs/big-data/build-and-orchestrate-etl-pipelines-using-amazon-athena-and-aws-step-functions/). AWS намагається показати, що вже має всі інструменти для побудови Lakehouse та може бути конкурентом Dremio, Upsolver та аналогічним продуктам. 

Даний кейс трохи притягнутий за вуха вибором технологій. Не дуже зрозуміле використання Athena для переміщення та простої трансформації даних між бакетом з вхідними даними та бакетом з вихідними даними. Athena має свої обмеження - у випадку складніших трансформацій прийдеться мігрувати на AWS Lambda або EMR/Glue ETL. Також прийдеться переплатити за дані, що просканує Athena для того аби зробити `INSERT INTO`, оскільки читання даних з допомогою Athena дорожче ніж читання напряму з S3.

Я би замінив Athena на AWS Lambda, якщо обробка даних вміщаються в ліміти Lambda. Якщо ні, то використав би AWS EMR/Glue в залежності від бюджету та доступної команди.